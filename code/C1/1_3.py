# %%
from dashscope_api import get_completion


# 三、局限性
# 大模型的幻觉问题
# 虚假知识：模型偶尔会生成一些看似真实实则编造的知识

# 在开发与应用语言模型时，需要注意它们可能生成虚假信息的风险。尽管模型经过大规模预训练，掌握了丰富知识，但它实际上并没有完全记住所见的信息，难以准确判断自己的知识边界，可能做出错误推断。若让语言模型描述一个不存在的产品,它可能会自行构造出似是而非的细节。这被称为“幻觉”(Hallucination)，是语言模型的一大缺陷。

# 如下示例展示了大模型的幻觉。我们要求告诉我们华为公司生产的 GT Watch 运动手表 产品的信息，事实上，这个公司是真实存在的，但产品是编造的，而模型一本正经地提供了它编造的知识，而且迷惑性很强。

# %%
prompt = f"""
请告诉我们华为公司生产的 GT Watch 运动手表 产品的信息。
"""
response = get_completion(prompt)
print(response)
# %%
